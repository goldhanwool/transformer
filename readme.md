# NumPy-based Transformer Model(Work in Progress)

This project aims to build a Transformer model from scratch using only the NumPy library in Python. The Transformer model is widely used in the field of Natural Language Processing (NLP) for tasks such as machine translation, text summarization, and other sequence-to-sequence applications. Initiated for educational purposes, this project serves as a resource for developers seeking to understand the inner workings of deep learning models through NumPy.

## Project Overview

The main goal of this project is to provide a clear and detailed implementation of the Transformer model, highlighting the core components such as attention mechanisms, positional encoding, and the overall architecture including encoder and decoder stacks. By focusing exclusively on NumPy, we aim to underscore the fundamental mathematics and algorithms that propel these models, offering profound insights into their functionality. It's imperative to recognize that mastering the Transformer architecture is key to unlocking a deeper understanding of modern NLP models, positioning it as an essential cornerstone for anyone delving into the field. This emphasis not only reflects the model's pivotal role in advancing NLP but also underscores its significance in shaping future innovations within the domain.

![Alt text](image/transformer.svg)
